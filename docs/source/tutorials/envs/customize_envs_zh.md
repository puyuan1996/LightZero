# LightZero 中如何自定义环境?

- 在使用 LightZero 进行强化学习的研究或应用时，可能需要创建自定义的环境。创建自定义环境可以更好地适应特定的问题或任务，使得强化学习算法能够在特定环境中进行有效的训练。
- 一个典型的 LightZero 中的环境，请参考 [atari_lightzero_env.py](https://github.com/opendilab/LightZero/blob/main/zoo/atari/envs/atari_lightzero_env.py)。LightZero的环境设计大致基于DI-engine的`BaseEnv`类。在创建自定义环境时，我们遵循了与DI-engine相似的基本步骤。以下是DI-engine中创建自定义环境的文档
  - https://di-engine-docs.readthedocs.io/zh_CN/latest/04_best_practice/ding_env_zh.html 

## 与 BaseEnv 的主要差异

为了让LightZero中的算法同时能够处理棋类与非棋类环境，LightZeroEnv 类需要兼容棋类与非棋类环境。棋类环境由于存在玩家交替执行动作，合法动作在变化的情况，需要在环境的 obs 中包含 action_mask、to_play 等信息。为了兼容性，对于非棋类环境，LightZero 同样要求环境返回的 'obs' 要包含相应的 action_mask、to_play 等信息。

LightZero 环境与 DI-engine 环境的区别主要体现在以下几点：

- 在 `reset()` 方法中，LightZeroEnv  返回的是一个字典 `lightzero_obs_dict = {'observation': obs, 'action_mask': action_mask, 'to_play': -1}`。
  - 对于非棋类环境
    - `to_play`的设置：由于非棋类环境一般只有一个玩家，因此设置 `to_play`=-1。(我们在算法中根据该值，判断执行单player的算法逻辑(`to_play`=-1)，还是多player的算法逻辑(`to_play`=N))
    - 对于action_mask的设置
      - 离散动作空间：`action_mask`= np.ones(self.env.action_space.n, 'int8') 是一个全1的numpy数组，表示所有动作都是合法动作。
      - 连续动作空间：`action_mask`= None，特殊的None表示环境是连续动作空间。
- 在 `step()` 方法中，返回的是 `BaseEnvTimestep(lightzero_obs_dict, rew, done, info)`，其中的 `lightzero_obs_dict` 包含了更新后的观察结果。

## 基本步骤

以下是创建自定义 LightZero 环境的基本步骤：

### 1. 创建环境类

首先，需要创建一个新的环境类，该类需要继承自 DI-engine 的 BaseEnv 类。例如：

```Python
from ding.envs import BaseEnv

class MyCustomEnv(BaseEnv):
    pass
```

### 2. 初始化方法

在自定义环境类中，需要定义一个初始化方法`__init__`。在这个方法中，需要设置一些环境的基本属性，例如观察空间、动作空间、奖励空间等。例如：

```Python
def __init__(self, cfg=None):
    self.cfg = cfg
    self._init_flag = False
    # set other properties...
```

### 3. Reset 方法

`reset`方法用于重置环境到一个初始状态。这个方法应该返回环境的初始观察。例如：

```Python
def reset(self):
    # reset the environment...
    obs = self._env.reset()
    # get the action_mask according to the legal action
    ...
    lightzero_obs_dict = {'observation': obs, 'action_mask': action_mask, 'to_play': -1}
    return lightzero_obs_dict
```

### 4. Step 方法

`step`方法接受一个动作作为输入，执行这个动作，并返回一个元组，包含新的观察、奖励、是否完成和其他信息。例如：

```Python
def step(self, action):
  # The core original env step.
    obs, rew, done, info = self.env.step(action)
    
    if self.cfg.continuous:
        action_mask = None
    else:
        # get the action_mask according to the legal action
        action_mask = np.ones(self.env.action_space.n, 'int8')
    
    lightzero_obs_dict = {'observation': obs, 'action_mask': action_mask, 'to_play': -1}
    
    self._eval_episode_return += rew
    if done:
        info['eval_episode_return'] = self._eval_episode_return
    
    return BaseEnvTimestep(lightzero_obs_dict, rew, done, info)
```

### 5. 观察空间和动作空间

在自定义环境中，需要提供观察空间和动作空间的属性。这些属性是 gym.Space 对象，描述了观察和动作的形状和类型。例如：

```Python
@property
def observation_space(self):
    return self._observation_space

@property
def action_space(self):
    return self._action_space
    
@property
def legal_actions(self):
    # get the actual legal actions
    return np.arange(self._action_space.n)
```

### 6. 其他方法

根据需要，可能还需要定义其他方法，例如`render`（用于显示环境的当前状态）、`close`（用于关闭环境并进行清理）等。

### 7. 注册环境

最后，需要使用 `ENV_REGISTRY.register` 装饰器来注册新的环境，使得可以在配置文件中使用它。例如：

```Python
from ding.utils import ENV_REGISTRY

@ENV_REGISTRY.register('my_custom_env')
class MyCustomEnv(BaseEnv):
    # ...
```

创建自定义环境可能需要对具体的任务和强化学习有深入的理解。在实现自定义环境时，可能需要进行一些试验和调整，以使环境能够有效地支持强化学习的训练。

## LightZeroEnvWrapper

- 我们在 lzero/envs/wrappers中提供了一个 [LightZeroEnvWrapper](https://github.com/opendilab/LightZero/blob/main/lzero/envs/wrappers/lightzero_env_wrapper.py)。它能够将经典的classic_control, box2d 环境包装成 LightZero 所需要的env格式。
- 具体使用时，使用下面的函数，将一个gym环境，通过LightZeroEnvWrapper包装成LightZero 所需要的env格式

```Python
def get_wrappered_env(wrapper_cfg: EasyDict, env_name: str):
    # overview comments
    ...
    if wrapper_cfg.manually_discretization:
        return lambda: DingEnvWrapper(
            gym.make(env_name),
            cfg={
                'env_wrapper': [
                    lambda env: ActionDiscretizationEnvWrapper(env, wrapper_cfg), lambda env:
                    LightZeroEnvWrapper(env, wrapper_cfg)
                ]
            }
        )
    else:
        return lambda: DingEnvWrapper(
            gym.make(env_name), cfg={'env_wrapper': [lambda env: LightZeroEnvWrapper(env, wrapper_cfg)]}
        )
```

然后在算法的主入口处中调用 train_muzero_with_gym_env 方法，即可使用上述包装后的 env 用于训练：

```Python
if __name__ == "__main__":
    """
    Overview:
        The ``train_muzero_with_gym_env`` entry means that the environment used in the training process is generated by wrapping the original gym environment with LightZeroEnvWrapper.
        Users can refer to lzero/envs/wrappers for more details.
    """
    from lzero.entry import train_muzero_with_gym_env
    train_muzero_with_gym_env([main_config, create_config], seed=0, max_env_step=max_env_step)
```

## 注意事项

- 状态表示：思考如何将环境状态表示为观察空间。对于简单的环境，可以直接使用低维连续状态；对于复杂的环境，可能需要使用图像或其他高维离散状态表示。
- 观察空间预处理：根据观察空间的类型，对输入数据进行适当的预处理操作，例如缩放、裁剪、灰度化、归一化等。预处理可以减少输入数据的维度，加速学习过程。
- 奖励设计：设计合理的符合目标的的奖励函数。例如，环境给出的外在奖励尽量归一化在[0, 1]。通过归一化环境给出的外在奖励，能更好的确定 RND 算法中的内在奖励权重等超参数。
